---
title: 'Modeling and Learning Dynamic Moral Judgments towards Allocations in High-stakes Domains'
collection: preprints
permalink: /publications/working-paper-1
venue: 'Major Revision, Management Science'
pubdate: September 2025
authors: 'Violet (Xinying) Chen, Joshua Williams, Derek Leben, Hoda Heidari'
---

[Download SSRN version here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5387617)

**Keywords: moral judgment modeling, moral preference learning, ethical principles**

Resource allocation in high-stakes domains, such as public health, education and employment, is critical for promoting long-term justice and equity. As AI tools are increasingly applied to assist these decisions, it is essential to model peoples' moral judgments to ensure their views on who and how allocation policies should prioritize and benefit are incorporated into AI systems. In this paper, we propose a framework for modeling and learning dynamic moral judgments in sequential resource allocations. Our approach is based on a Markov Decision Process (MDP) model, where the state reward function characterizes a stakeholder's moral preferences over allocation policies. To reflect shifts in ethical principles of allocations, we define piecewise linear reward functions to account for the changing moral priorities. We design a reward learning algorithm that actively queries stakeholders for feedback, using which the approximate reward function is iteratively updated with Bayesian inference. We illustrate our model through simulation experiment and human subject study focused on the allocation of scarce medical resource distributions during a hypothetical viral epidemic. The simulations demonstrate the effectiveness and validity of our approach, while the human subject study illustrates its applicability in interacting with participants to describe their moral judgments.

<!--

---
title: 'Online Convex Optimization Perspective for Learning from Dynamically Revealed Preferences'
collection: preprints
permalink: /publications/2020-08-23-preprint
venue: 'ArXiv: 2008.10460'
pubdate: 2021
authors: 'Violet (Xinying) Chen, Fatma Kılınç-Karzan'
---

[Download paper here](https://vxychen.github.io/files/OCO_Perspective_for_Learning_from_Dynamically_Revealed_Preferences.pdf)

**Keywords: online inverse learning, online convex optimization, preference elicitation**

We study the problem of online learning (OL) from revealed preferences: a learner wishes to learn a non-strategic agent’s private utility function through observing the agent’s utility-maximizing actions in a changing environment. We adopt an online inverse optimization setup, where the learner observes
a stream of agent’s actions in an online fashion and the learning performance is measured by regret associated with a loss function. We first characterize a special but broad class of agent’s utility functions, then utilize this structure in designing a new convex loss function. We establish that the regret with respect to our new loss function also bounds the regret with respect to three classical loss functions commonly used in the inverse optimization literature. This allows us to design a flexible OL framework that enables a unified treatment of loss functions and supports a variety of online convex optimization algorithms. We demonstrate with theoretical and empirical evidence that our framework based on the new loss function (in particular online Mirror Descent) has significant advantages in terms of regret performance and solution time over other OL algorithms from the literature and bypasses the previous technical assumptions as well.

-->
